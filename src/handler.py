import os
import shutil
from src import cfg
from src import data_proccessing as dp
from src import optuna_e as optuna
from utils import json_parser





class Handler:
    def __init__(self, working_dir):
        self.working_dir = working_dir
        self.tmp_dir = os.path.join(working_dir, "temp")
        self.testing_zip = os.path.join(working_dir, "testing.zip")
        self.training_zip = os.path.join(working_dir, "training.zip")
        self.testing_json = os.path.join(working_dir, cfg.testing_embeddings)
        self.training_json = os.path.join(working_dir, cfg.training_embeddings)
        self.result_csv = os.path.join(working_dir, cfg.results_folder)
        self.setup()
        
    def setup(self):
        if os.path.exists(self.tmp_dir):
            shutil.rmtree(self.tmp_dir)
            
        self.makeExistent(self.testing_json)
        self.makeExistent(self.training_json)
        self.makeExistent(self.result_csv)
            
    
    def makeExistent(self, path):
        if not os.path.exists(path):
            os.mkdir(path)
        
    def createJSON(self, training, encoding):
        folder = self.training_zip if training else self.testing_zip
        d = dp.DataProcessing(folder, encoding)
        return d.createdPath

    def optimize(self, encodedJSON, opt_func, normalize):
        name = os.path.join(self.training_json, encodedJSON)
        optuna.Evaluator(self.working_dir, name, opt_func, normalize)
        
    def analyzeJSON(self, encodedJSON, training=True):
        folder = self.training_json if training else self.testing_json
        json_parser.analyze_json(os.path.join(folder, encodedJSON))
        
    def optimizeALL(self, opt_functions, normalizations):
        for file in os.listdir(self.training_json):
            for opt_func in opt_functions:
                for normalize in normalizations:
                    self.optimize(file, opt_func, normalize)
                    
    def generateSolution(self, encodedJSON, model):
        name = os.path.join(self.training_json, encodedJSON)
        data = dp.DataProcessing(name)
        labels = model(data.dataPoints)
    
        path = os.path.join(self.result_csv, "output_" + model.__name__ + ".csv")
        with open(path, "w") as f:
            for name, label in zip(data.names, labels):
                f.write(f"{name};{label}\n")
                

        


