from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.svm import OneClassSVM
from sklearn.tree import DecisionTreeClassifier


def adaBoostClasifier_model(trial, trainData, testData, trainLabels, testLabels):
    n_estimators = trial.suggest_int('n_estimators', 1, 100)
    learning_rate = trial.suggest_float('learning_rate', 0.1, 2.0)
    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])
    
    clf = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, algorithm=algorithm)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def randomForestClasifier_model(trial, trainData, testData, trainLabels, testLabels):
    n_estimators = trial.suggest_int('n_estimators', 1, 100)
    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])
    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])
    min_impurity_decrease = trial.suggest_float('min_impurity_decrease', 0.0, 0.5)
    
    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_features=max_features, min_impurity_decrease=min_impurity_decrease)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def gaussianProcessClasifier_model(trial, trainData, testData, trainLabels, testLabels):
    kernel = trial.suggest_categorical('kernel', ['RBF', 'DotProduct', 'Matern', 'RationalQuadratic', 'WhiteKernel'])
    max_iter_predict = trial.suggest_int('max_iter_predict', 100, 200)
    
    clf = GaussianProcessClassifier(kernel=kernel, max_iter_predict=max_iter_predict)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def gaussianNB_model(trial, trainData, testData, trainLabels, testLabels):
    var_smoothing = trial.suggest_float('var_smoothing', 1e-09, 1e-05)
    
    clf = GaussianNB(var_smoothing=var_smoothing)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def kNeighborsClassifier_model(trial, trainData, testData, trainLabels, testLabels):
    n_neighbors = trial.suggest_int('n_neighbors', 1, 100)
    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])
    algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])
    leaf_size = trial.suggest_int('leaf_size', 1, 100)
    p = trial.suggest_int('p', 1, 100)
    
    clf = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, leaf_size=leaf_size, p=p)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def mLPClassifier_model(trial, trainData, testData, trainLabels, testLabels):
    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(100,), (100, 100), (100, 100, 100)])
    activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])
    solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])
    alpha = trial.suggest_float('alpha', 0.0001, 0.5)
    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])
    max_iter = trial.suggest_int('max_iter', 100, 200)
    
    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, alpha=alpha, learning_rate=learning_rate, max_iter=max_iter)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def svc_model(trial, trainData, testData):
    C = trial.suggest_float('C', 0.1, 2.0)
    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
    degree = trial.suggest_int('degree', 1, 100)
    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
    coef0 = trial.suggest_float('coef0', 0.0, 1.0)
    shrinking = trial.suggest_categorical('shrinking', [True, False])
    tol = trial.suggest_float('tol', 0.001, 0.1)
    max_iter = trial.suggest_int('max_iter', 100, 200)
    
    clf = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, shrinking=shrinking, tol=tol, max_iter=max_iter)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)
    
def decisionTreeClasifier_model(trial, trainData, testData, trainLabels, testLabels):
    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])
    splitter = trial.suggest_categorical('splitter', ['best', 'random'])
    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])
    min_impurity_decrease = trial.suggest_float('min_impurity_decrease', 0.0, 0.5)
    
    clf = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_features=max_features, min_impurity_decrease=min_impurity_decrease)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)

def oneClassSVM_model(trial, trainData, testData, trainLabels, testLabels):
    nu = trial.suggest_float('nu', 0.1, 1.0)
    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
    degree = trial.suggest_int('degree', 1, 100)
    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
    coef0 = trial.suggest_float('coef0', 0.0, 1.0)
    shrinking = trial.suggest_categorical('shrinking', [True, False])
    tol = trial.suggest_float('tol', 0.001, 0.1)
    max_iter = trial.suggest_int('max_iter', 100, 200)
    
    clf = OneClassSVM(nu=nu, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, shrinking=shrinking, tol=tol, max_iter=max_iter)
    clf.fit(trainData, trainLabels)
    return clf.score(testData, testLabels)






